{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyP6uUKZbN5MujBCb6OjP6TT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2WTCft9J9a2G","executionInfo":{"status":"ok","timestamp":1676177457457,"user_tz":-540,"elapsed":17239,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}},"outputId":"84ac51ff-4f7d-4365-b447-9b6e67522df7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","os.getcwd()\n","os.chdir('/content/drive/MyDrive/015GithubRepos/Gitbook_recsys/PinSage')"],"metadata":{"id":"_jlQuyVx9qPX","executionInfo":{"status":"ok","timestamp":1676177572696,"user_tz":-540,"elapsed":280,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["os.getcwd()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"9clDleyA-JaK","executionInfo":{"status":"ok","timestamp":1676177614740,"user_tz":-540,"elapsed":4,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}},"outputId":"49e2b04a-40ec-4694-eb22-2b76b5242e80"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/015GithubRepos/Gitbook_recsys/PinSage'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["from pathlib import Path\n","print(Path())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4H3SuEAQ-Leu","executionInfo":{"status":"ok","timestamp":1676177942429,"user_tz":-540,"elapsed":4,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}},"outputId":"36261fbd-3fd2-4457-feaf-2945875e6fef"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":[".\n"]}]},{"cell_type":"code","source":["print(Path().is_file())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CS_t9dlF_a-d","executionInfo":{"status":"ok","timestamp":1676177968226,"user_tz":-540,"elapsed":325,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}},"outputId":"a5d76a30-ef46-4ae3-b86a-2578a6d398c1"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"code","source":["print(Path(os.getcwd()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w1vgWXbP_h0H","executionInfo":{"status":"ok","timestamp":1676178352527,"user_tz":-540,"elapsed":3,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}},"outputId":"9e00110d-6ec0-42b6-ee5e-168aef294137"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/015GithubRepos/Gitbook_recsys/PinSage\n"]}]},{"cell_type":"code","source":["cwd = Path(os.getcwd())"],"metadata":{"id":"gFgCOZJ2_hwl","executionInfo":{"status":"ok","timestamp":1676178372396,"user_tz":-540,"elapsed":3,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["print(cwd.parent.joinpath('data', 'ml-1m'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"71b54IgA_hsx","executionInfo":{"status":"ok","timestamp":1676178528883,"user_tz":-540,"elapsed":3,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}},"outputId":"a54eb6be-e777-4321-823a-1e4e0ff9dabc"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/015GithubRepos/Gitbook_recsys/data/ml-1m\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"XEU7QxcK_hpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pR88K9dH_hlX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ai0iXGdb_hhd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import re"],"metadata":{"id":"SOkHWiQZ_hPN","executionInfo":{"status":"ok","timestamp":1676178785623,"user_tz":-540,"elapsed":3,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["data_path = Path(os.getcwd()).parent.joinpath('data', 'ml-1m')\n","\n","data_file_name = 'users.dat'\n","users = []\n","with open(data_path.joinpath(data_file_name), encoding=\"latin1\") as f:\n","    for l in f:\n","        id_, gender, age, occupation, zip_ = l.strip().split(\"::\")\n","        users.append(\n","            {\n","                \"user_id\": int(id_),\n","                \"gender\": gender,\n","                \"age\": age,\n","                \"occupation\": occupation,\n","                \"zip\": zip_,\n","            }\n","        )\n","users = pd.DataFrame(users).astype(\"category\")\n","\n","data_file_name = 'movies.dat'\n","movies = []\n","with open(data_path.joinpath(data_file_name), encoding=\"latin1\") as f:\n","    for l in f:\n","        id_, title, genres = l.strip().split(\"::\")\n","        genres_set = set(genres.split(\"|\"))\n","\n","        # extract year\n","        assert re.match(r\".*\\([0-9]{4}\\)$\", title)\n","        year = title[-5:-1]\n","        title = title[:-6].strip()\n","\n","        data = {\"movie_id\": int(id_), \"title\": title, \"year\": year}\n","        for g in genres_set:\n","            data[g] = True\n","        movies.append(data)\n","movies = pd.DataFrame(movies).astype({\"year\": \"category\"})\n","\n","data_file_name = 'ratings.dat'\n","ratings = []\n","with open(data_path.joinpath(data_file_name), encoding=\"latin1\") as f:\n","    for l in f:\n","        user_id, movie_id, rating, timestamp = [\n","            int(_) for _ in l.split(\"::\")\n","        ]\n","        ratings.append(\n","            {\n","                \"user_id\": user_id,\n","                \"movie_id\": movie_id,\n","                \"rating\": rating,\n","                \"timestamp\": timestamp,\n","            }\n","        )\n","ratings = pd.DataFrame(ratings)"],"metadata":{"id":"xadF_6ED9eF8","executionInfo":{"status":"ok","timestamp":1676178790492,"user_tz":-540,"elapsed":4287,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["# Filter the users and items that never appear in the rating table.\n","distinct_users_in_ratings = ratings[\"user_id\"].unique()\n","distinct_movies_in_ratings = ratings[\"movie_id\"].unique()\n","users = users[users[\"user_id\"].isin(distinct_users_in_ratings)]\n","movies = movies[movies[\"movie_id\"].isin(distinct_movies_in_ratings)]"],"metadata":{"id":"CZj2o27i-D4L","executionInfo":{"status":"ok","timestamp":1676179581553,"user_tz":-540,"elapsed":329,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["# Group the movie features into genres (a vector), year (a category), title (a string)\n","genre_columns = movies.columns.drop([\"movie_id\", \"title\", \"year\"])\n","movies[genre_columns] = movies[genre_columns].fillna(False).astype(\"bool\")\n","movies_categorical = movies.drop(\"title\", axis=1)"],"metadata":{"id":"rzaNJLobFqeg","executionInfo":{"status":"ok","timestamp":1676179591901,"user_tz":-540,"elapsed":298,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["pip install dgl"],"metadata":{"id":"r3NiLikzGE05"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Graph builder from pandas dataframes\"\"\"\n","from collections import namedtuple\n","\n","from pandas.api.types import (\n","    is_categorical,\n","    is_categorical_dtype,\n","    is_numeric_dtype,\n",")\n","\n","import dgl\n","\n","__all__ = [\"PandasGraphBuilder\"]\n","\n","\n","def _series_to_tensor(series):\n","    if is_categorical(series):\n","        return torch.LongTensor(series.cat.codes.values.astype(\"int64\"))\n","    else:  # numeric\n","        return torch.FloatTensor(series.values)\n","\n","\n","class PandasGraphBuilder(object):\n","    \"\"\"Creates a heterogeneous graph from multiple pandas dataframes.\n","\n","    Examples\n","    --------\n","    Let's say we have the following three pandas dataframes:\n","\n","    User table ``users``:\n","\n","    ===========  ===========  =======\n","    ``user_id``  ``country``  ``age``\n","    ===========  ===========  =======\n","    XYZZY        U.S.         25\n","    FOO          China        24\n","    BAR          China        23\n","    ===========  ===========  =======\n","\n","    Game table ``games``:\n","\n","    ===========  =========  ==============  ==================\n","    ``game_id``  ``title``  ``is_sandbox``  ``is_multiplayer``\n","    ===========  =========  ==============  ==================\n","    1            Minecraft  True            True\n","    2            Tetris 99  False           True\n","    ===========  =========  ==============  ==================\n","\n","    Play relationship table ``plays``:\n","\n","    ===========  ===========  =========\n","    ``user_id``  ``game_id``  ``hours``\n","    ===========  ===========  =========\n","    XYZZY        1            24\n","    FOO          1            20\n","    FOO          2            16\n","    BAR          2            28\n","    ===========  ===========  =========\n","\n","    One could then create a bidirectional bipartite graph as follows:\n","    >>> builder = PandasGraphBuilder()\n","    >>> builder.add_entities(users, 'user_id', 'user')\n","    >>> builder.add_entities(games, 'game_id', 'game')\n","    >>> builder.add_binary_relations(plays, 'user_id', 'game_id', 'plays')\n","    >>> builder.add_binary_relations(plays, 'game_id', 'user_id', 'played-by')\n","    >>> g = builder.build()\n","    >>> g.num_nodes('user')\n","    3\n","    >>> g.num_edges('plays')\n","    4\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.entity_tables = {}\n","        self.relation_tables = {}\n","\n","        self.entity_pk_to_name = (\n","            {}\n","        )  # mapping from primary key name to entity name\n","        self.entity_pk = {}  # mapping from entity name to primary key\n","        self.entity_key_map = (\n","            {}\n","        )  # mapping from entity names to primary key values\n","        self.num_nodes_per_type = {}\n","        self.edges_per_relation = {}\n","        self.relation_name_to_etype = {}\n","        self.relation_src_key = {}  # mapping from relation name to source key\n","        self.relation_dst_key = (\n","            {}\n","        )  # mapping from relation name to destination key\n","\n","    def add_entities(self, entity_table, primary_key, name):\n","        entities = entity_table[primary_key].astype(\"category\")\n","        if not (entities.value_counts() == 1).all():\n","            raise ValueError(\n","                \"Different entity with the same primary key detected.\"\n","            )\n","        # preserve the category order in the original entity table\n","        entities = entities.cat.reorder_categories(\n","            entity_table[primary_key].values\n","        )\n","\n","        self.entity_pk_to_name[primary_key] = name\n","        self.entity_pk[name] = primary_key\n","        self.num_nodes_per_type[name] = entity_table.shape[0]\n","        self.entity_key_map[name] = entities\n","        self.entity_tables[name] = entity_table\n","\n","    def add_binary_relations(\n","        self, relation_table, source_key, destination_key, name\n","    ):\n","        src = relation_table[source_key].astype(\"category\")\n","        src = src.cat.set_categories(\n","            self.entity_key_map[\n","                self.entity_pk_to_name[source_key]\n","            ].cat.categories\n","        )\n","        dst = relation_table[destination_key].astype(\"category\")\n","        dst = dst.cat.set_categories(\n","            self.entity_key_map[\n","                self.entity_pk_to_name[destination_key]\n","            ].cat.categories\n","        )\n","        if src.isnull().any():\n","            raise ValueError(\n","                \"Some source entities in relation %s do not exist in entity %s.\"\n","                % (name, source_key)\n","            )\n","        if dst.isnull().any():\n","            raise ValueError(\n","                \"Some destination entities in relation %s do not exist in entity %s.\"\n","                % (name, destination_key)\n","            )\n","\n","        srctype = self.entity_pk_to_name[source_key]\n","        dsttype = self.entity_pk_to_name[destination_key]\n","        etype = (srctype, name, dsttype)\n","        self.relation_name_to_etype[name] = etype\n","        self.edges_per_relation[etype] = (\n","            src.cat.codes.values.astype(\"int64\"),\n","            dst.cat.codes.values.astype(\"int64\"),\n","        )\n","        self.relation_tables[name] = relation_table\n","        self.relation_src_key[name] = source_key\n","        self.relation_dst_key[name] = destination_key\n","\n","    def build(self):\n","        # Create heterograph\n","        graph = dgl.heterograph(\n","            self.edges_per_relation, self.num_nodes_per_type\n","        )\n","        return graph"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cbkb6bXmF_NB","executionInfo":{"status":"ok","timestamp":1676179712369,"user_tz":-540,"elapsed":3529,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}},"outputId":"5fad00ed-9a34-4484-be87-b2cb22f900b6"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stderr","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n"]},{"output_type":"stream","name":"stdout","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"]}]},{"cell_type":"code","source":["# Build graph\n","graph_builder = PandasGraphBuilder()\n","graph_builder.add_entities(users, \"user_id\", \"user\")\n","graph_builder.add_entities(movies_categorical, \"movie_id\", \"movie\")\n","graph_builder.add_binary_relations(\n","    ratings, \"user_id\", \"movie_id\", \"watched\"\n",")\n","graph_builder.add_binary_relations(\n","    ratings, \"movie_id\", \"user_id\", \"watched-by\"\n",")\n","\n","g = graph_builder.build()"],"metadata":{"id":"r_WXX4wqFtAz","executionInfo":{"status":"ok","timestamp":1676179717106,"user_tz":-540,"elapsed":244,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["import torch"],"metadata":{"id":"Lr41r--4GX6x","executionInfo":{"status":"ok","timestamp":1676179770514,"user_tz":-540,"elapsed":3,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["import dask.dataframe as dd\n","import numpy as np\n","import scipy.sparse as ssp\n","import torch\n","import tqdm\n","\n","import dgl\n","\n","\n","# This is the train-test split method most of the recommender system papers running on MovieLens\n","# takes.  It essentially follows the intuition of \"training on the past and predict the future\".\n","# One can also change the threshold to make validation and test set take larger proportions.\n","def train_test_split_by_time(df, timestamp, user):\n","    df[\"train_mask\"] = np.ones((len(df),), dtype=np.bool)\n","    df[\"val_mask\"] = np.zeros((len(df),), dtype=np.bool)\n","    df[\"test_mask\"] = np.zeros((len(df),), dtype=np.bool)\n","    df = dd.from_pandas(df, npartitions=10)\n","\n","    def train_test_split(df):\n","        df = df.sort_values([timestamp])\n","        if df.shape[0] > 1:\n","            df.iloc[-1, -3] = False\n","            df.iloc[-1, -1] = True\n","        if df.shape[0] > 2:\n","            df.iloc[-2, -3] = False\n","            df.iloc[-2, -2] = True\n","        return df\n","\n","    df = (\n","        df.groupby(user, group_keys=False)\n","        .apply(train_test_split)\n","        .compute(scheduler=\"processes\")\n","        .sort_index()\n","    )\n","    print(df[df[user] == df[user].unique()[0]].sort_values(timestamp))\n","    return (\n","        df[\"train_mask\"].to_numpy().nonzero()[0],\n","        df[\"val_mask\"].to_numpy().nonzero()[0],\n","        df[\"test_mask\"].to_numpy().nonzero()[0],\n","    )\n","\n","\n","def build_train_graph(g, train_indices, utype, itype, etype, etype_rev):\n","    train_g = g.edge_subgraph(\n","        {etype: train_indices, etype_rev: train_indices}, relabel_nodes=False\n","    )\n","\n","    # copy features\n","    for ntype in g.ntypes:\n","        for col, data in g.nodes[ntype].data.items():\n","            train_g.nodes[ntype].data[col] = data\n","    for etype in g.etypes:\n","        for col, data in g.edges[etype].data.items():\n","            train_g.edges[etype].data[col] = data[\n","                train_g.edges[etype].data[dgl.EID]\n","            ]\n","\n","    return train_g\n","\n","\n","def build_val_test_matrix(g, val_indices, test_indices, utype, itype, etype):\n","    n_users = g.num_nodes(utype)\n","    n_items = g.num_nodes(itype)\n","    val_src, val_dst = g.find_edges(val_indices, etype=etype)\n","    test_src, test_dst = g.find_edges(test_indices, etype=etype)\n","    val_src = val_src.numpy()\n","    val_dst = val_dst.numpy()\n","    test_src = test_src.numpy()\n","    test_dst = test_dst.numpy()\n","    val_matrix = ssp.coo_matrix(\n","        (np.ones_like(val_src), (val_src, val_dst)), (n_users, n_items)\n","    )\n","    test_matrix = ssp.coo_matrix(\n","        (np.ones_like(test_src), (test_src, test_dst)), (n_users, n_items)\n","    )\n","\n","    return val_matrix, test_matrix\n","\n","\n","def linear_normalize(values):\n","    return (values - values.min(0, keepdims=True)) / (\n","        values.max(0, keepdims=True) - values.min(0, keepdims=True)\n","    )"],"metadata":{"id":"OcfRcj7wGhQ-","executionInfo":{"status":"ok","timestamp":1676179878224,"user_tz":-540,"elapsed":264,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["import dask.dataframe as dd\n","import numpy as np\n","import scipy.sparse as ssp\n","import torch\n","import tqdm\n","\n","import dgl\n"],"metadata":{"id":"wFHgykfhGkBg","executionInfo":{"status":"ok","timestamp":1676179889494,"user_tz":-540,"elapsed":291,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["# Assign features.\n","# Note that variable-sized features such as texts or images are handled elsewhere.\n","g.nodes[\"user\"].data[\"gender\"] = torch.LongTensor(\n","    users[\"gender\"].cat.codes.values\n",")\n","g.nodes[\"user\"].data[\"age\"] = torch.LongTensor(\n","    users[\"age\"].cat.codes.values\n",")\n","g.nodes[\"user\"].data[\"occupation\"] = torch.LongTensor(\n","    users[\"occupation\"].cat.codes.values\n",")\n","g.nodes[\"user\"].data[\"zip\"] = torch.LongTensor(\n","    users[\"zip\"].cat.codes.values\n",")\n","\n","g.nodes[\"movie\"].data[\"year\"] = torch.LongTensor(\n","    movies[\"year\"].cat.codes.values\n",")\n","g.nodes[\"movie\"].data[\"genre\"] = torch.FloatTensor(\n","    movies[genre_columns].values\n",")\n","\n","g.edges[\"watched\"].data[\"rating\"] = torch.LongTensor(\n","    ratings[\"rating\"].values\n",")\n","g.edges[\"watched\"].data[\"timestamp\"] = torch.LongTensor(\n","    ratings[\"timestamp\"].values\n",")\n","g.edges[\"watched-by\"].data[\"rating\"] = torch.LongTensor(\n","    ratings[\"rating\"].values\n",")\n","g.edges[\"watched-by\"].data[\"timestamp\"] = torch.LongTensor(\n","    ratings[\"timestamp\"].values\n",")\n","\n","# Train-validation-test split\n","# This is a little bit tricky as we want to select the last interaction for test, and the\n","# second-to-last interaction for validation.\n","train_indices, val_indices, test_indices = train_test_split_by_time(\n","    ratings, \"timestamp\", \"user_id\"\n",")\n","\n","# Build the graph with training interactions only.\n","train_g = build_train_graph(\n","    g, train_indices, \"user\", \"movie\", \"watched\", \"watched-by\"\n",")\n","assert train_g.out_degrees(etype=\"watched\").min() > 0\n","\n","# Build the user-item sparse matrix for validation and test set.\n","val_matrix, test_matrix = build_val_test_matrix(\n","    g, val_indices, test_indices, \"user\", \"movie\", \"watched\"\n",")\n","\n","## Build title set\n","\n","movie_textual_dataset = {\"title\": movies[\"title\"].values}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JHz6tb0XFwz2","executionInfo":{"status":"ok","timestamp":1676179900166,"user_tz":-540,"elapsed":10290,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}},"outputId":"f3642a54-8939-4eff-a9c0-926d3e7aeb85"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-65-0ab4e594acee>:14: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  df[\"train_mask\"] = np.ones((len(df),), dtype=np.bool)\n","<ipython-input-65-0ab4e594acee>:15: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  df[\"val_mask\"] = np.zeros((len(df),), dtype=np.bool)\n","<ipython-input-65-0ab4e594acee>:16: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  df[\"test_mask\"] = np.zeros((len(df),), dtype=np.bool)\n","<ipython-input-65-0ab4e594acee>:30: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n","  Before: .apply(func)\n","  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n","  or:     .apply(func, meta=('x', 'f8'))            for series result\n","  df.groupby(user, group_keys=False)\n"]},{"output_type":"stream","name":"stdout","text":["    user_id  movie_id  rating  timestamp  train_mask  val_mask  test_mask\n","31        1      3186       4  978300019        True     False      False\n","27        1      1721       4  978300055        True     False      False\n","37        1      1022       5  978300055        True     False      False\n","22        1      1270       5  978300055        True     False      False\n","24        1      2340       3  978300103        True     False      False\n","36        1      1836       5  978300172        True     False      False\n","3         1      3408       4  978300275        True     False      False\n","47        1      1207       4  978300719        True     False      False\n","7         1      2804       5  978300719        True     False      False\n","21        1       720       3  978300760        True     False      False\n","0         1      1193       5  978300760        True     False      False\n","44        1       260       4  978300760        True     False      False\n","9         1       919       4  978301368        True     False      False\n","51        1       608       4  978301398        True     False      False\n","43        1      2692       4  978301570        True     False      False\n","41        1      1961       5  978301590        True     False      False\n","48        1      2028       5  978301619        True     False      False\n","18        1      3105       5  978301713        True     False      False\n","11        1       938       4  978301752        True     False      False\n","42        1      1962       4  978301753        True     False      False\n","14        1      1035       5  978301753        True     False      False\n","39        1       150       5  978301777        True     False      False\n","17        1      2018       4  978301777        True     False      False\n","45        1      1028       5  978301777        True     False      False\n","26        1      1097       4  978301953        True     False      False\n","2         1       914       3  978301968        True     False      False\n","19        1      2797       4  978302039        True     False      False\n","6         1      1287       5  978302039        True     False      False\n","38        1      2762       4  978302091        True     False      False\n","52        1      1246       4  978302091        True     False      False\n","1         1       661       3  978302109        True     False      False\n","13        1      2918       4  978302124        True     False      False\n","49        1       531       4  978302149        True     False      False\n","50        1      3114       4  978302174        True     False      False\n","15        1      2791       4  978302188        True     False      False\n","46        1      1029       5  978302205        True     False      False\n","20        1      2321       3  978302205        True     False      False\n","5         1      1197       3  978302268        True     False      False\n","8         1       594       4  978302268        True     False      False\n","12        1      2398       4  978302281        True     False      False\n","28        1      1545       4  978824139        True     False      False\n","23        1       527       5  978824195        True     False      False\n","40        1         1       5  978824268        True     False      False\n","33        1       588       4  978824268        True     False      False\n","16        1      2687       3  978824268        True     False      False\n","29        1       745       3  978824268        True     False      False\n","10        1       595       5  978824268        True     False      False\n","30        1      2294       4  978824291        True     False      False\n","35        1       783       4  978824291        True     False      False\n","4         1      2355       5  978824291        True     False      False\n","34        1      1907       4  978824330        True     False      False\n","32        1      1566       4  978824330       False      True      False\n","25        1        48       5  978824351       False     False       True\n"]}]},{"cell_type":"code","source":["# The model should build their own vocabulary and process the texts.  Here is one example\n","# of using torchtext to pad and numericalize a batch of strings.\n","#     field = torchtext.data.Field(include_lengths=True, lower=True, batch_first=True)\n","#     examples = [torchtext.data.Example.fromlist([t], [('title', title_field)]) for t in texts]\n","#     titleset = torchtext.data.Dataset(examples, [('title', title_field)])\n","#     field.build_vocab(titleset.title, vectors='fasttext.simple.300d')\n","#     token_ids, lengths = field.process([examples[0].title, examples[1].title])\n","\n","## Dump the graph and the datasets\n","import pickle\n","out_directory = 'prep_data'\n","dgl.save_graphs(os.path.join(out_directory, \"train_g.bin\"), train_g)\n","\n","dataset = {\n","    \"val-matrix\": val_matrix,\n","    \"test-matrix\": test_matrix,\n","    \"item-texts\": movie_textual_dataset,\n","    \"item-images\": None,\n","    \"user-type\": \"user\",\n","    \"item-type\": \"movie\",\n","    \"user-to-item-type\": \"watched\",\n","    \"item-to-user-type\": \"watched-by\",\n","    \"timestamp-edge-column\": \"timestamp\",\n","}\n","\n","with open(os.path.join(out_directory, \"data.pkl\"), \"wb\") as f:\n","    pickle.dump(dataset, f)"],"metadata":{"id":"t8bT44emGW5-","executionInfo":{"status":"ok","timestamp":1676180015307,"user_tz":-540,"elapsed":387,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["for _ in cwd.glob('**/*'):\n","    print(_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lfjCT7h0HBDO","executionInfo":{"status":"ok","timestamp":1676180089412,"user_tz":-540,"elapsed":232,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}},"outputId":"4203b8a1-f955-428a-920d-0926f47cb094"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/015GithubRepos/Gitbook_recsys/PinSage/pinsage_main\n","/content/drive/MyDrive/015GithubRepos/Gitbook_recsys/PinSage/prep_data\n","/content/drive/MyDrive/015GithubRepos/Gitbook_recsys/PinSage/prep_data/train_g.bin\n","/content/drive/MyDrive/015GithubRepos/Gitbook_recsys/PinSage/prep_data/data.pkl\n"]}]},{"cell_type":"code","source":["import argparse\n","import os\n","import pickle\n","\n","import evaluation\n","import layers\n","import numpy as np\n","import sampler as sampler_module\n","import torch\n","import torch.nn as nn\n","import torchtext\n","import tqdm\n","from torch.utils.data import DataLoader\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","import dgl"],"metadata":{"id":"ScpcQkcwHdf4","executionInfo":{"status":"ok","timestamp":1676180241471,"user_tz":-540,"elapsed":278,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["class PinSAGEModel(nn.Module):\n","    def __init__(self, full_graph, ntype, textsets, hidden_dims, n_layers):\n","        super().__init__()\n","\n","        self.proj = layers.LinearProjector(\n","            full_graph, ntype, textsets, hidden_dims\n","        )\n","        self.sage = layers.SAGENet(hidden_dims, n_layers)\n","        self.scorer = layers.ItemToItemScorer(full_graph, ntype)\n","\n","    def forward(self, pos_graph, neg_graph, blocks):\n","        h_item = self.get_repr(blocks)\n","        pos_score = self.scorer(pos_graph, h_item)\n","        neg_score = self.scorer(neg_graph, h_item)\n","        return (neg_score - pos_score + 1).clamp(min=0)\n","\n","    def get_repr(self, blocks):\n","        h_item = self.proj(blocks[0].srcdata)\n","        h_item_dst = self.proj(blocks[-1].dstdata)\n","        return h_item_dst + self.sage(blocks, h_item)\n","\n","\n","def train(dataset, args):\n","    g = dataset[\"train-graph\"]\n","    val_matrix = dataset[\"val-matrix\"].tocsr()\n","    test_matrix = dataset[\"test-matrix\"].tocsr()\n","    item_texts = dataset[\"item-texts\"]\n","    user_ntype = dataset[\"user-type\"]\n","    item_ntype = dataset[\"item-type\"]\n","    user_to_item_etype = dataset[\"user-to-item-type\"]\n","    timestamp = dataset[\"timestamp-edge-column\"]\n","\n","    device = torch.device(args.device)\n","\n","    # Assign user and movie IDs and use them as features (to learn an individual trainable\n","    # embedding for each entity)\n","    g.nodes[user_ntype].data[\"id\"] = torch.arange(g.num_nodes(user_ntype))\n","    g.nodes[item_ntype].data[\"id\"] = torch.arange(g.num_nodes(item_ntype))\n","\n","    # Prepare torchtext dataset and Vocabulary\n","    textset = {}\n","    tokenizer = get_tokenizer(None)\n","\n","    textlist = []\n","    batch_first = True\n","\n","    for i in range(g.num_nodes(item_ntype)):\n","        for key in item_texts.keys():\n","            l = tokenizer(item_texts[key][i].lower())\n","            textlist.append(l)\n","    for key, field in item_texts.items():\n","        vocab2 = build_vocab_from_iterator(\n","            textlist, specials=[\"<unk>\", \"<pad>\"]\n","        )\n","        textset[key] = (\n","            textlist,\n","            vocab2,\n","            vocab2.get_stoi()[\"<pad>\"],\n","            batch_first,\n","        )\n","\n","    # Sampler\n","    batch_sampler = sampler_module.ItemToItemBatchSampler(\n","        g, user_ntype, item_ntype, args.batch_size\n","    )\n","    neighbor_sampler = sampler_module.NeighborSampler(\n","        g,\n","        user_ntype,\n","        item_ntype,\n","        args.random_walk_length,\n","        args.random_walk_restart_prob,\n","        args.num_random_walks,\n","        args.num_neighbors,\n","        args.num_layers,\n","    )\n","    collator = sampler_module.PinSAGECollator(\n","        neighbor_sampler, g, item_ntype, textset\n","    )\n","    dataloader = DataLoader(\n","        batch_sampler,\n","        collate_fn=collator.collate_train,\n","        num_workers=args.num_workers,\n","    )\n","    dataloader_test = DataLoader(\n","        torch.arange(g.num_nodes(item_ntype)),\n","        batch_size=args.batch_size,\n","        collate_fn=collator.collate_test,\n","        num_workers=args.num_workers,\n","    )\n","    dataloader_it = iter(dataloader)\n","\n","    # Model\n","    model = PinSAGEModel(\n","        g, item_ntype, textset, args.hidden_dims, args.num_layers\n","    ).to(device)\n","    # Optimizer\n","    opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","    # For each batch of head-tail-negative triplets...\n","    for epoch_id in range(args.num_epochs):\n","        model.train()\n","        for batch_id in tqdm.trange(args.batches_per_epoch):\n","            pos_graph, neg_graph, blocks = next(dataloader_it)\n","            # Copy to GPU\n","            for i in range(len(blocks)):\n","                blocks[i] = blocks[i].to(device)\n","            pos_graph = pos_graph.to(device)\n","            neg_graph = neg_graph.to(device)\n","\n","            loss = model(pos_graph, neg_graph, blocks).mean()\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","\n","        # Evaluate\n","        model.eval()\n","        with torch.no_grad():\n","            item_batches = torch.arange(g.num_nodes(item_ntype)).split(\n","                args.batch_size\n","            )\n","            h_item_batches = []\n","            for blocks in dataloader_test:\n","                for i in range(len(blocks)):\n","                    blocks[i] = blocks[i].to(device)\n","\n","                h_item_batches.append(model.get_repr(blocks))\n","            h_item = torch.cat(h_item_batches, 0)\n","\n","            print(\n","                evaluation.evaluate_nn(dataset, h_item, args.k, args.batch_size)\n","            )"],"metadata":{"id":"KV0FMCxQHzmP","executionInfo":{"status":"ok","timestamp":1676180269598,"user_tz":-540,"elapsed":380,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"dataset_path\", type=str)\n","parser.add_argument(\"--random-walk-length\", type=int, default=2)\n","parser.add_argument(\"--random-walk-restart-prob\", type=float, default=0.5)\n","parser.add_argument(\"--num-random-walks\", type=int, default=10)\n","parser.add_argument(\"--num-neighbors\", type=int, default=3)\n","parser.add_argument(\"--num-layers\", type=int, default=2)\n","parser.add_argument(\"--hidden-dims\", type=int, default=16)\n","parser.add_argument(\"--batch-size\", type=int, default=32)\n","parser.add_argument(\n","    \"--device\", type=str, default=\"cpu\"\n",")  # can also be \"cuda:0\"\n","parser.add_argument(\"--num-epochs\", type=int, default=1)\n","parser.add_argument(\"--batches-per-epoch\", type=int, default=20000)\n","parser.add_argument(\"--num-workers\", type=int, default=0)\n","parser.add_argument(\"--lr\", type=float, default=3e-5)\n","parser.add_argument(\"-k\", type=int, default=10)\n","\n","args = parser.parse_args([])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RbZZqdUOIScc","executionInfo":{"status":"ok","timestamp":1676180533210,"user_tz":-540,"elapsed":287,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}},"outputId":"128015c0-bb7f-4502-afdc-6de7376b2da5"},"execution_count":90,"outputs":[{"output_type":"execute_result","data":{"text/plain":["_StoreAction(option_strings=['-k'], dest='k', nargs=None, const=None, default=10, type=<class 'int'>, choices=None, help=None, metavar=None)"]},"metadata":{},"execution_count":90}]},{"cell_type":"code","source":["cwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4qXhqOIHKiO2","executionInfo":{"status":"ok","timestamp":1676180861302,"user_tz":-540,"elapsed":262,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}},"outputId":"0da2675d-a5f9-49ae-f16a-fe17b36ca143"},"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PosixPath('/content/drive/MyDrive/015GithubRepos/Gitbook_recsys/PinSage')"]},"metadata":{},"execution_count":97}]},{"cell_type":"code","source":["import easydict\n"," \n","args = easydict.EasyDict({\n"," \n","        \"dataset_path\": cwd.joinpath('prep_data'),\n","        \"random_walk_length\" : 2,\n","        'random_walk_restart_prob' : 0.5,\n","        'num_random_walks' : 10,\n","        'num_neighbors' : 3,\n","        'num_layers' : 2,\n","        'hidden_dims' : 16,\n","        'batch_size' : 32,\n","        'device' : 'cpu',\n","        \"num_epochs\": 1,\n","        'batches_per_epoch' : 20000,\n","        'num_workers' : 0,\n","        'lr' : 3e-5,\n","        'k' : 10\n"," \n","})"],"metadata":{"id":"wNaDdP9bJBMa","executionInfo":{"status":"ok","timestamp":1676180916452,"user_tz":-540,"elapsed":332,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}}},"execution_count":98,"outputs":[]},{"cell_type":"code","source":["# Load dataset\n","data_info_path = os.path.join(args.dataset_path, \"data.pkl\")\n","with open(data_info_path, \"rb\") as f:\n","    dataset = pickle.load(f)\n","train_g_path = os.path.join(args.dataset_path, \"train_g.bin\")\n","g_list, _ = dgl.load_graphs(train_g_path)\n","dataset[\"train-graph\"] = g_list[0]\n","train(dataset, args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IlAPkqH3IXgG","executionInfo":{"status":"ok","timestamp":1676181980266,"user_tz":-540,"elapsed":1063414,"user":{"displayName":"KYEONGCHAN LEE","userId":"03106579917275952793"}},"outputId":"63bbf1bb-2d16-41e7-ab7f-5ce81f9d2baf"},"execution_count":99,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 20000/20000 [17:39<00:00, 18.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.03394039735099338\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wAmaXDB3KWVu"},"execution_count":null,"outputs":[]}]}