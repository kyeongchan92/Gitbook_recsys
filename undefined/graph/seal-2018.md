# SEAL(2018)

## Paper

[Zhang, M., & Chen, Y. (2018). Link prediction based on graph neural networks. Advances in neural information processing systems, 31.](https://arxiv.org/abs/1802.09691)

official code : [https://github.com/muhanzhang/SEAL](https://github.com/muhanzhang/SEAL)

## 1 Introduction <a href="#1-introduction" id="1-introduction"></a>

기존의 Link prediction 방법은 heuristic 방법이었다. heuristic 방법이란 heuristic node similarity scores를 계산하여 링크를 확률로 바라보는 방법이다.

**What is heuristic?** A heuristic function h(n), takes a node n and returns a _**non-negative real number**_ that is an estimate of the cost of the least-cost path from node n to a goal node.

즉, 최단거리로 이동할 때 드는 비용을 양의 실수로 표현하는 것이다.

## 2 Preliminaries <a href="#2-preliminaries" id="2-preliminaries"></a>

$$\Gamma(x)$$: x의 1-hop 이웃

$$d(x,y)$$ : 가장 짧은 거리

## 3 A theory for unifying link prediction heuristics <a href="#3-a-theory-for-unifying-link-prediction-heuristics" id="3-a-theory-for-unifying-link-prediction-heuristics"></a>

**Definition 1. (Enclosing subgraph)**

**Theorem 1. Any h-order heuristic for (x, y) can be accurately calculated from Gx,yh**

high-order heuristic은 여기서 정의하는 γ-decaying heuristic을 이용하면 작은 h로도 근사될 수 있다.

**Definition 2. (γ-decaying heuristic)**

$$
\mathcal{H} = \eta \sum_{l=1}^{\infty} \gamma^l f(x, y, l)
$$

γ는 0에서 1 사이 값을 가진 decaying factor이다. η는 양의 상수이다. f is a nonnegative function. l은 walk의 길이.

**Theorem 2. H(x,y) can be approximated from Gx,yh**

**Lemma 1. Any walk between x and y with length l≤2h+1 is included in Gx,yh.**

three popular high-order heuristics: Katz, rooted PageRank and SimRank satisfy Theorem 2.

l≤2h+1x와 y사이의 길이가 l 이하인 워크, 그 워크에 포함된 노드들은 무조건 서브그래프 Gx,yh에 포함된다.

### 3.1 Katz index <a href="#31-katz-index" id="31-katz-index"></a>

$$
\begin{align*}
\text{Katz}_{x,y} =& \sum_{l=1}^{\infty} \beta^l |\text{walks}^{\left\langle l \right\rangle} (x, y)|  \\
=& \sum_{l=1}^{\infty} \beta^l \left[ A^l \right]_{x,y}
\end{align*}
$$

그래프가 주어지면 바로 계산되어 나옴. Katz index가 높은 노드끼리 연결하면 됨.

$$\eta = 1$$, $$\gamma = \beta$$라고 보면 $$\gamma$$-decaying heuristic으로 일반화 될 수 있음.

**Proposition 1. 인접행렬의** $$l$$**제곱 행렬의 모든 값은** $$d^l$$**보다 작다.** $$d$$**는 degree중 최고값이다.**

### 3.2 PageRank <a href="#32-pagerank" id="32-pagerank"></a>

the score for $$(x, y)$$ is given by $$[πx]y$$

x, y의 h-hop subgraph를 뽑은 후, l ≤ 2h-1을 만족하는 l에 대하여 f가 계산될 수 잇음. 이 값은

### 3.3 SimRank <a href="#33-simrank" id="33-simrank"></a>

sim(x, y)는 recursive하게 정의된다.

**Theorem 4. SimRank는** $$\gamma$$**-decaying heuristic을 만족함.**

## 4 SEAL <a href="#4-seal" id="4-seal"></a>

SEAL은 세 가지 스텝으로 이루어진다.

1. Enclosing subgraph 추출
2. 노드 정보 행렬 구축
3. Graph Neural Network 학습

2번의 노드 정보 행렬 $$X$$는 다음 세 가지로 이루어져 있다.

1. 구조적 노드 라벨 <-- 논문에서 설명해줌
2. 노드 임베딩 <-- Node2vec으로 얻어진 노드의 임베딩값
3. 노드의 속성

함수(모델)은 타겟 링크의 주변 enclosing subgraph를 인풋으로 받아서 타겟 링크가 존재할 확률을 아웃풋으로 내놓는다. 함수(모델)을 학습시키기 위해서 enclosing graph를 GNN에 대해 학습시킨다. 그러므로 SEAL의 첫 번째 스텝은 positive 링크와 negative link들의 enclosing graph를 추출하여 학습 데이터를 만드는 것이다.

GNN은 일반적으로 $$(A, X)$$를 인풋으로 취한다. $$A$$는 enclosing subgraph의 인접행렬이고, $$A$$는 _노드 정보 행렬_이다. 노드 정보 행렬의 각 행은 노드의 피쳐 벡터이다.

### 4.1 Node labeling <a href="#41-node-labeling" id="41-node-labeling"></a>

노드 라벨링은 subgraph내의 모든 노드 i에 자연수 $$f_l(i)$$를 할당하는 함수 $$f_l : V \rightarrow \mathbb{N}$$이다. 제안하는 방법은 enclosing subgraph 내 노드들의 서로 다른 역할에 따라 다른 라벨을 마킹하는 것이다.

1\) 센터 노드 x, y는 예측하고자 하는 링크가 사이에 위치한 타겟 노드이다. 2) 센터 노드로부터 상대적 위치가 다른 노드들은 해당 링크에 갖는 구조적 중요성도 다르다.

적절한 노드 라벨링은 이러한 차이를 마킹할 수 있어야 한다. 만약 이러한 차이를 마킹하지 않으면 GNN은 예측해야할 링크를 가질 타겟 노드가 어디있는지 알지 못하며, 구조적 중요성도 잃어버린다.

제안하는 노드 라벨링 방법은 아래와 같은 기준을 따라 도출되었다:

1\) 두 타겟 노드 x와 y는 항상 라벨 1을 갖는다.

2\) $$d(i, x)=d(j,x)$$와 $$d(i,y)=d(j,y)$$를 만족하는 노드 i와 j는 같은 라벨을 갖는다.

enclosing subgraph에서 노드 i가 갖는 위상학적인 위치는 두 센터 노드 관점에서의 _**반경**_에 의해 설명될 수 있기 때문이다. 그러므로, 같은 궤도에 위치한 노드에 같은 라벨을 부여하여 노드라벨이 상대적 위치와 구조적 중요성을 반영할 수 있도록 하였다.

![node\_labeling\_example](https://wikidocs.net/images/page/178479/node\_labeling\_example.png)

위 기준에 기반하여, Double-Radius Node Labeling (DRNL)을 제안한다. 우선, $$(d(i, x), d(i, y)) = (1, 1)$$을 만족하는 모든 노드 i에게 라벨 $$f_l(i)=2$$를 할당한다. 반경 (1,2) 또는 (2,1)을 만족하는 노드에게 3을 할당한다. (1,3) 또는 (3,1)을 만족하는 노드에게 4를 할당한다. (2, 2)를 만족하는 노드에게 5를 할당한다. (1, 4) 또는 (4, 1)을 만족하는 노드에게 6을 할당한다. (2, 3) 또는 (3, 2)를 만족하는 노드에게 7을 할당한다. 이런 방식으로 반복적으로 라벨을 할당한다. 즉, 반경이 클 수록 더 큰 수의 라벨을 할당한다. 라벨 $$f_l(i)$$와 double-radius (d(i,x), d(i,y)는 아래를 만족한다.

1\) 만약 $$d(i, x)+d(i,y) \ne d(j,x) + d(j,y)$$라면, $$d(i,x) + d(i,y) < d(j,x) + d(j,y) \Leftrightarrow f_l(i) < f_l(j)$$;

2\) 만약 $$d(i, x)+d(i,y) = d(j,x) + d(j,y)$$라면, $$d(i,x)d(i,y) < d(j,x)d(j,y) \Leftrightarrow f_l(i) < f_l(j)$$

DRNL의 장점 중 하나는, 이 방법이 완벽한 해싱 함수를 갖고 있다는 것이다:

$$
f_l(i) = 1 + \min(d_x, d_y) + (d/2)[(d/2) + (d\%2) - 1]
$$

$$d_x := d(i,x)$$, $$d_y := d(i, y)$$, $$d := d_x + d_y$$, $$(d/2)$$와 $$(d\%2)$$는 각각 d를 2로 나눈 몫과 나머지를 의미한다.

$$d(i, x)=\infty$$ 또는 $$d(i, y)=\infty$$인 노드들에게는 0을 부여한다. DRNL이 유일한 노드 라벨링 방법이 아니다. 그러나 경험적으로 다른 라벨링 방법보다 더 좋은 성능을 보인다는 것이 알려져있다. 라벨을 얻은 후, 이 라벨으 원한 인코딩을 이용하여 X를 구축한다.

### 4.2 Incorporating latent and explicit features <a href="#42-incorporating-latent-and-explicit-features" id="42-incorporating-latent-and-explicit-features"></a>

노드라벨만큼, 노드 정보 행렬 X도 latent 및 explicit 피쳐 정보를 포함할 수 있다. X의 행 방향으로 임베딩/속성 벡터를 컨캣함으로써 위 세 가지의 정보를 모두 학습할 수 있다.

### 4.3 Negative injection <a href="#43-negative-injection" id="43-negative-injection"></a>

SEAL을 위한 노드임베딩을 생성하는것은 중요하다. 관측된 그래프 $$G=(V, E)$$, 샘플링된 positive training 링크 $$E_p \subseteq E$$, 샘플링된 negative training 링크 $$E_n \subseteq E$$이 주어졌다고 가정해보자. 만약 우리가 G로부터 직접 노드 임베딩을 생성한다면, 노드임베딩은 training 링크의 존재 정보를 갖게 된다. 우리는 GNN이 그러한 존재 정보를 빠르게 찾아내어 최적화한다는 것을 발견했다. 그러나 이는 실험에서 일반화 성능을 떨어트렸다. 우리의 트릭은 $$E_n$$을 $$E$$에 추가하여 $$G'=(V, E \cup E_n)$$을 만드는 것이다. 이 방식으로 하면 positive 및 negative training 링크는 임베딩 속에서 동일한 링크 정보를 갖게 되어, GNN은 정보의 이 부분만 적합시켜 링크를 분류할 수 없다(?). 우리는 경험적으로 이 SEAL에 이 트릭을 사용한 것이 더 나은 성능을 보이는 것을 확인했다.

### Link Prediction <a href="#link-prediction" id="link-prediction"></a>

input : 노드라벨링이 완료된 subgraph, subgraph 내 노드의 연결관계(인접행렬의 일부), subgraph 내 노드의 피쳐 임베딩

output : 0 \~ 1 사이의 확률
