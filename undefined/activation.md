# activation의 역할

## [What are "Activations", "Activation Gradients", "Weights" and "Weight Gradients" in Convolutional Neural Networks?](https://stackoverflow.com/questions/57038055/what-are-activations-activation-gradients-weights-and-weight-gradients)

ReLU는 비선형 함수가 아닌데 왜 쓰는가?

<figure><img src="../.gitbook/assets/image.png" alt=""><figcaption></figcaption></figure>
