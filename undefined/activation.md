# activation의 역할(작성중)

ReLU는 비선형 함수가 아닌데 왜 쓰는가?

{% embed url="https://blog.paperspace.com/vanishing-gradients-activation-function/" %}

<figure><img src="../.gitbook/assets/image (4) (4).png" alt=""><figcaption></figcaption></figure>
