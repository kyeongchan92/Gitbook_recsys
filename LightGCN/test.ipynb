{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyeongchanlee/.pyenv/versions/3.9.9/envs/candidate/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from options import args\n",
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "from enum import Enum\n",
    "# from parse import parse_args\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "all_dataset = ['lastfm', 'gowalla', 'yelp2018', 'amazon-book']\n",
    "all_models  = ['mf', 'lgn']\n",
    "# config['batch_size'] = 4096\n",
    "config['bpr_batch_size'] = args.bpr_batch\n",
    "config['latent_dim_rec'] = args.recdim\n",
    "config['lightGCN_n_layers']= args.layer\n",
    "config['dropout'] = args.dropout\n",
    "config['keep_prob']  = args.keepprob\n",
    "config['A_n_fold'] = args.a_fold\n",
    "config['test_u_batch_size'] = args.testbatch\n",
    "config['multicore'] = args.multicore\n",
    "config['lr'] = args.lr\n",
    "config['decay'] = args.decay\n",
    "config['pretrain'] = args.pretrain\n",
    "config['A_split'] = False\n",
    "config['bigdata'] = False\n",
    "\n",
    "GPU = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if GPU else \"cpu\")\n",
    "CORES = multiprocessing.cpu_count() // 2\n",
    "seed = args.seed\n",
    "\n",
    "dataset = args.dataset\n",
    "model_name = args.model\n",
    "\n",
    "# TRAIN_epochs = args.epochs\n",
    "LOAD = args.load\n",
    "PATH = args.path\n",
    "topks = eval(args.topks)\n",
    "tensorboard = args.tensorboard\n",
    "comment = args.comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Loader(Dataset):\n",
    "    def __init__(self,config = world.config, path=\"../data/amazon-book\"):\n",
    "        # train or test\n",
    "        cprint(f'loading [{path}]')\n",
    "        self.split = config['A_split']\n",
    "        self.folds = config['A_n_fold']\n",
    "        self.mode_dict = {'train': 0, \"test\": 1}\n",
    "        self.mode = self.mode_dict['train']\n",
    "        self.n_user = 0\n",
    "        self.m_item = 0\n",
    "        train_file = path + '/train.txt'\n",
    "        test_file = path + '/test.txt'\n",
    "        self.path = path\n",
    "        trainUniqueUsers, trainItem, trainUser = [], [], []\n",
    "        testUniqueUsers, testItem, testUser = [], [], []\n",
    "        self.traindataSize = 0\n",
    "        self.testDataSize = 0\n",
    "\n",
    "        with open(train_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    trainUniqueUsers.append(uid)\n",
    "                    trainUser.extend([uid] * len(items))\n",
    "                    trainItem.extend(items)\n",
    "                    self.m_item = max(self.m_item, max(items))\n",
    "                    self.n_user = max(self.n_user, uid)\n",
    "                    self.traindataSize += len(items)\n",
    "        self.trainUniqueUsers = np.array(trainUniqueUsers)\n",
    "        self.trainUser = np.array(trainUser)\n",
    "        self.trainItem = np.array(trainItem)\n",
    "\n",
    "        with open(test_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    testUniqueUsers.append(uid)\n",
    "                    testUser.extend([uid] * len(items))\n",
    "                    testItem.extend(items)\n",
    "                    self.m_item = max(self.m_item, max(items))\n",
    "                    self.n_user = max(self.n_user, uid)\n",
    "                    self.testDataSize += len(items)\n",
    "        self.m_item += 1\n",
    "        self.n_user += 1\n",
    "        self.testUniqueUsers = np.array(testUniqueUsers)\n",
    "        self.testUser = np.array(testUser)\n",
    "        self.testItem = np.array(testItem)\n",
    "        \n",
    "        self.Graph = None\n",
    "        print(f\"{self.trainDataSize} interactions for training\")\n",
    "        print(f\"{self.testDataSize} interactions for testing\")\n",
    "        print(f\"{world.dataset} Sparsity : {(self.trainDataSize + self.testDataSize) / self.n_users / self.m_items}\")\n",
    "\n",
    "        # (users,items), bipartite graph\n",
    "        self.UserItemNet = csr_matrix((np.ones(len(self.trainUser)), (self.trainUser, self.trainItem)),\n",
    "                                      shape=(self.n_user, self.m_item))\n",
    "        self.users_D = np.array(self.UserItemNet.sum(axis=1)).squeeze()\n",
    "        self.users_D[self.users_D == 0.] = 1\n",
    "        self.items_D = np.array(self.UserItemNet.sum(axis=0)).squeeze()\n",
    "        self.items_D[self.items_D == 0.] = 1.\n",
    "        # pre-calculate\n",
    "        self._allPos = self.getUserPosItems(list(range(self.n_user)))\n",
    "        self.__testDict = self.__build_test()\n",
    "        print(f\"{world.dataset} is ready to go\")\n",
    "\n",
    "    @property\n",
    "    def n_users(self):\n",
    "        return self.n_user\n",
    "    \n",
    "    @property\n",
    "    def m_items(self):\n",
    "        return self.m_item\n",
    "    \n",
    "    @property\n",
    "    def trainDataSize(self):\n",
    "        return self.traindataSize\n",
    "    \n",
    "    @property\n",
    "    def testDict(self):\n",
    "        return self.__testDict\n",
    "\n",
    "    @property\n",
    "    def allPos(self):\n",
    "        return self._allPos\n",
    "\n",
    "    def _split_A_hat(self,A):\n",
    "        A_fold = []\n",
    "        fold_len = (self.n_users + self.m_items) // self.folds\n",
    "        for i_fold in range(self.folds):\n",
    "            start = i_fold*fold_len\n",
    "            if i_fold == self.folds - 1:\n",
    "                end = self.n_users + self.m_items\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "            A_fold.append(self._convert_sp_mat_to_sp_tensor(A[start:end]).coalesce().to(world.device))\n",
    "        return A_fold\n",
    "\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        row = torch.Tensor(coo.row).long()\n",
    "        col = torch.Tensor(coo.col).long()\n",
    "        index = torch.stack([row, col])\n",
    "        data = torch.FloatTensor(coo.data)\n",
    "        return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n",
    "        \n",
    "    def getSparseGraph(self):\n",
    "        print(\"loading adjacency matrix\")\n",
    "        if self.Graph is None:\n",
    "            try:\n",
    "                pre_adj_mat = sp.load_npz(self.path + '/s_pre_adj_mat.npz')\n",
    "                print(\"successfully loaded...\")\n",
    "                norm_adj = pre_adj_mat\n",
    "            except :\n",
    "                print(\"generating adjacency matrix\")\n",
    "                s = time()\n",
    "                adj_mat = sp.dok_matrix((self.n_users + self.m_items, self.n_users + self.m_items), dtype=np.float32)\n",
    "                adj_mat = adj_mat.tolil()\n",
    "                R = self.UserItemNet.tolil()\n",
    "                adj_mat[:self.n_users, self.n_users:] = R\n",
    "                adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "                adj_mat = adj_mat.todok()\n",
    "                # adj_mat = adj_mat + sp.eye(adj_mat.shape[0])\n",
    "                \n",
    "                rowsum = np.array(adj_mat.sum(axis=1))\n",
    "                d_inv = np.power(rowsum, -0.5).flatten()\n",
    "                d_inv[np.isinf(d_inv)] = 0.\n",
    "                d_mat = sp.diags(d_inv)\n",
    "                \n",
    "                norm_adj = d_mat.dot(adj_mat)\n",
    "                norm_adj = norm_adj.dot(d_mat)\n",
    "                norm_adj = norm_adj.tocsr()\n",
    "                end = time()\n",
    "                print(f\"costing {end-s}s, saved norm_mat...\")\n",
    "                sp.save_npz(self.path + '/s_pre_adj_mat.npz', norm_adj)\n",
    "\n",
    "            if self.split == True:\n",
    "                self.Graph = self._split_A_hat(norm_adj)\n",
    "                print(\"done split matrix\")\n",
    "            else:\n",
    "                self.Graph = self._convert_sp_mat_to_sp_tensor(norm_adj)\n",
    "                self.Graph = self.Graph.coalesce().to(world.device)\n",
    "                print(\"don't split the matrix\")\n",
    "        return self.Graph\n",
    "\n",
    "    def __build_test(self):\n",
    "        \"\"\"\n",
    "        return:\n",
    "            dict: {user: [items]}\n",
    "        \"\"\"\n",
    "        test_data = {}\n",
    "        for i, item in enumerate(self.testItem):\n",
    "            user = self.testUser[i]\n",
    "            if test_data.get(user):\n",
    "                test_data[user].append(item)\n",
    "            else:\n",
    "                test_data[user] = [item]\n",
    "        return test_data\n",
    "\n",
    "    def getUserItemFeedback(self, users, items):\n",
    "        \"\"\"\n",
    "        users:\n",
    "            shape [-1]\n",
    "        items:\n",
    "            shape [-1]\n",
    "        return:\n",
    "            feedback [-1]\n",
    "        \"\"\"\n",
    "        # print(self.UserItemNet[users, items])\n",
    "        return np.array(self.UserItemNet[users, items]).astype('uint8').reshape((-1,))\n",
    "\n",
    "    def getUserPosItems(self, users):\n",
    "        posItems = []\n",
    "        for user in users:\n",
    "            posItems.append(self.UserItemNet[user].nonzero()[1])\n",
    "        return posItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRLoss:\n",
    "    def __init__(self, recmodel, config : dict):\n",
    "        self.model = recmodel\n",
    "        self.weight_decay = config['decay']\n",
    "        self.lr = config['lr']\n",
    "        self.opt = optim.Adam(recmodel.parameters(), lr=self.lr)\n",
    "\n",
    "    def stageOne(self, users, pos, neg):\n",
    "        loss, reg_loss = self.model.bpr_loss(users, pos, neg)\n",
    "        reg_loss = reg_loss*self.weight_decay\n",
    "        loss = loss + reg_loss\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        return loss.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def BPR_train_original(dataset, recommend_model, loss_class, epoch, neg_k=1, w=None):\n",
    "Recmodel = recommend_model\n",
    "Recmodel.train()\n",
    "bpr: utils.BPRLoss = loss_class\n",
    "\n",
    "with timer(name=\"Sample\"):\n",
    "    S = utils.UniformSample_original(dataset)\n",
    "users = torch.Tensor(S[:, 0]).long()\n",
    "posItems = torch.Tensor(S[:, 1]).long()\n",
    "negItems = torch.Tensor(S[:, 2]).long()\n",
    "\n",
    "users = users.to(world.device)\n",
    "posItems = posItems.to(world.device)\n",
    "negItems = negItems.to(world.device)\n",
    "users, posItems, negItems = utils.shuffle(users, posItems, negItems)\n",
    "total_batch = len(users) // world.config['bpr_batch_size'] + 1\n",
    "aver_loss = 0.\n",
    "for (batch_i,\n",
    "        (batch_users,\n",
    "        batch_pos,\n",
    "        batch_neg)) in enumerate(utils.minibatch(users,\n",
    "                                                posItems,\n",
    "                                                negItems,\n",
    "                                                batch_size=world.config['bpr_batch_size'])):\n",
    "    cri = bpr.stageOne(batch_users, batch_pos, batch_neg)\n",
    "    aver_loss += cri\n",
    "    if world.tensorboard:\n",
    "        w.add_scalar(f'BPRLoss/BPR', cri, epoch * int(len(users) / world.config['bpr_batch_size']) + batch_i)\n",
    "aver_loss = aver_loss / total_batch\n",
    "time_info = timer.dict()\n",
    "timer.zero()\n",
    "    return f\"loss{aver_loss:.3f}-{time_info}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyeongchanlee/.pyenv/versions/3.9.9/envs/candidate/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import LightGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LightGCN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m LightGCN()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LightGCN' is not defined"
     ]
    }
   ],
   "source": [
    "model = LightGCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(args.epochs):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "bpr = utils.BPRLoss(Recmodel, world.config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "candidate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c9a439ae93980d4049c8b7e2fc8dfa6026c40a268f63cc6d8c2833fbd13c5bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
